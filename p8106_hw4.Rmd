---
title: "Homework 4"
author: "Adeline Shin"
date: "4/26/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lasso2)
library(ISLR)
library(caret)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)
library(randomForest)
library(gbm)
library(plotmo)
library(pdp)
library(lime)
library(ModelMetrics)

set.seed(1)
```

# Problem 1
## Loading and Cleaning the Data
```{r}
data("Prostate")
prostate_df = Prostate
```

## Part A: Regression Tree
```{r}
# Building the initial tree
tree_1 = rpart(formula = lpsa ~ ., data = prostate_df)
rpart.plot(tree_1)

# Pruning to determine size of best tree
cpTable = printcp(tree_1)
plotcp(tree_1)
minErr = which.min(cpTable[, 4])

# Tree with minimum CV
min_cv_tree = prune(tree_1, cp = cpTable[minErr, 1])
rpart.plot(min_cv_tree)

# Tree with 1 SE rule
se_tree = prune(tree_1, cp = cpTable[cpTable[, 4] < cpTable[minErr, 4]+cpTable[minErr, 5], 1][1])
rpart.plot(se_tree)
```

As shown above, the tree obtained with the minimum CV is not the same as that obtained with the 1 SE rule. The tree that corresponds to the minimum cross-validation error has a size of 8, while the tree obtained with the 1 SE rule only has a size of 3.

## Part B: Final Tree Plot
Looking above at the cp plot from Part A, the left-most cp with the mean relative error below the horizontal line is cp = 0.1. Therefore, this value will be used to plot the final chosen tree, which is the same as the tree obtained using the 1 SE rule.

```{r}
final_tree = rpart(formula = lpsa ~ ., data = prostate_df, control = rpart.control(cp = 0.1))
rpart.plot(final_tree)
```

The left-most node on this tree means that of 

## Part C: Bagging
```{r}
set.seed(1)
bagging = randomForest(lpsa ~ ., data = prostate_df, mtry = 8)
bagging$importance
```

The matrix of variable importances is shown above, with the values signifying each of the variable's importance in the random forest model.

## Part D: Random Forest
```{r}
set.seed(1)
rf = randomForest(lpsa ~ ., data = prostate_df, mtry = 2)
rf$importance
```

The variable importance matrix above shows that the value of importance changes between the random forest model and the bagging model done in Part C.

## Part E: Boosting
```{r}
set.seed(1)
boosting = gbm(lpsa ~ ., data = prostate_df,
               distribution = "gaussian",
               n.trees = 5000,
               interaction.depth = 3,
               shrinkage = 0.005,
               cv.folds = 10)

nt = gbm.perf(boosting, method = "cv")
```

According to the graph above, the number of trees that should be added to the ensemble is `r nt`. In addition, the graph below shows the variable importance in the bagging model.

```{r}
summary(boosting, las = 2, cBars = 19, cex.names = 0.6)
```

## Part F: Model Comparison
```{r}

```


# Problem 2
## Loading and Cleaning the Data
```{r}
data(OJ)
oj_data = OJ %>% 
  janitor::clean_names()

set.seed(1)
rowTrain = createDataPartition(y = oj_data$pct_disc_ch,
                               p = 800/1070,
                               list = FALSE)
training_data = as.data.frame(oj_data[rowTrain,])
test_data = as.data.frame(oj_data[-rowTrain,])
```

## Part A: Classification Forest
```{r}
# Initial classification tree
set.seed(1)
class_tree = rpart(formula = purchase ~ ., data = oj_data,
                   subset = rowTrain,
                   control = rpart.control(cp = 0))
```

Using the cross-validation error plot, the final tree with the minimum error is shown below.

```{r}
cpTable = printcp(class_tree)
plotcp(class_tree)
minErr = which.min(cpTable[,4])

# Using minimum cross-validation error
tree2 = prune(class_tree, cp = cpTable[minErr,1])
rpart.plot(tree2)
```

The response prediction can be shown below, as well as the MSE calculation as the test error.

```{r}
rpart_predict = predict(class_tree, newdata = test_data)
rpart_mse = mse(rpart_predict, test_data$purchase)
```

When using the training data to predict the response, the test classification MSE is `r rpart_mse`, which means the classification error rate is `r rpart_mse * 100`%.

## Part B: Random Forest
```{r}
set.seed(1)
rf_2_final = ranger::ranger(purchase ~ ., data = oj_data,
                            mtry = 10,
                            splitrule = "gini",
                            min.node.size = 5,
                            importance = "permutation",
                            scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(rf_2_final), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred", "white", "darkblue"))(19))
```

The test error rate is

```{r}

```


## Part C: Boosting
```{r}
set.seed(1)
boosting_2 = gbm(purchase ~ ., data = oj_data,
                 distribution = "gaussian",
                 n.trees = 5000,
                 interaction.depth = 3,
                 shrinkage = 0.005,
                 cv.folds = 10)
```

```{r}
nt_2 = gbm.perf(boosting_2, method = "cv")
```

The graph above shows the number of trees that should be included, which is `r nt_2`. Below, a bar graph of the relative variable importance is shown for the boosting model.

```{r}
summary(boosting_2, las = 2, cBars = 19, cex.names = 0.6)
```

The test error rate 